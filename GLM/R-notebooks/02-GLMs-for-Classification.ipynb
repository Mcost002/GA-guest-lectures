{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - GLMs for Classification\n",
    "GLMs make excellent, flexible statistical models.  However, they also excel at making classification algorithms.  The purpose of this notebook is to illustrate how this might be done in R.\n",
    "\n",
    "I give three examples of how GLMs may be used for classification:\n",
    "* Logistic regression\n",
    "* Multinomial logistic regression\n",
    "* Cumulative logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 5, repr.plot.height = 3)\n",
    "library(ggplot2)\n",
    "library(VGAM)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/Timothy/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n",
      "also installing the dependencies 'bindr', 'assertthat', 'bindrcpp', 'glue', 'pkgconfig', 'plogr'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'bindr' successfully unpacked and MD5 sums checked\n",
      "package 'assertthat' successfully unpacked and MD5 sums checked\n",
      "package 'bindrcpp' successfully unpacked and MD5 sums checked\n",
      "package 'glue' successfully unpacked and MD5 sums checked\n",
      "package 'pkgconfig' successfully unpacked and MD5 sums checked\n",
      "package 'plogr' successfully unpacked and MD5 sums checked\n",
      "package 'dplyr' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\Timothy\\AppData\\Local\\Temp\\RtmpUxN6MG\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages('dplyr', repos = \"http://cran.us.r-project.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Classification\n",
    "#### Data:\n",
    "For this example, I use the same data set from part 01: the `grad` data.  This is from [UCLA's IDRE module](https://stats.idre.ucla.edu/r/dae/logit-regression/), and here's its description:\n",
    "\n",
    " _A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/donâ€™t admit, is a binary variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1 \n",
       "273 127 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(grad$admit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a training/test split\n",
    "set.seed(1234)\n",
    "n_train <- 100\n",
    "index_train <- sample(1:nrow(iris), n_train, replace = FALSE)\n",
    "grad_train <- grad[index_train,]\n",
    "grad_test <- grad[-index_train,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = admit ~ gre + gpa + rank, family = binomial, data = grad_train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-1.9378  -0.7644  -0.4646   0.9375   2.3371  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -0.163788   2.303155  -0.071  0.94331    \n",
       "gre          0.006096   0.002436   2.502  0.01234 *  \n",
       "gpa         -0.566717   0.721434  -0.786  0.43214    \n",
       "rank        -0.887582   0.253785  -3.497  0.00047 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 128.21  on 99  degrees of freedom\n",
       "Residual deviance: 105.59  on 96  degrees of freedom\n",
       "AIC: 113.59\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Produce the same model\n",
    "glm_logit <- glm(admit ~ gre + gpa + rank, data = grad_train, family = binomial)\n",
    "summary(glm_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>18</dt>\n",
       "\t\t<dd>0.110781011996383</dd>\n",
       "\t<dt>93</dt>\n",
       "\t\t<dd>0.674281009824169</dd>\n",
       "\t<dt>91</dt>\n",
       "\t\t<dd>0.539340247665819</dd>\n",
       "\t<dt>92</dt>\n",
       "\t\t<dd>0.781574319954265</dd>\n",
       "\t<dt>126</dt>\n",
       "\t\t<dd>0.0880398246197096</dd>\n",
       "\t<dt>149</dt>\n",
       "\t\t<dd>0.556147060365041</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[18] 0.110781011996383\n",
       "\\item[93] 0.674281009824169\n",
       "\\item[91] 0.539340247665819\n",
       "\\item[92] 0.781574319954265\n",
       "\\item[126] 0.0880398246197096\n",
       "\\item[149] 0.556147060365041\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "18\n",
       ":   0.11078101199638393\n",
       ":   0.67428100982416991\n",
       ":   0.53934024766581992\n",
       ":   0.781574319954265126\n",
       ":   0.0880398246197096149\n",
       ":   0.556147060365041\n",
       "\n"
      ],
      "text/plain": [
       "        18         93         91         92        126        149 \n",
       "0.11078101 0.67428101 0.53934025 0.78157432 0.08803982 0.55614706 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict training/test response\n",
    "yhat_train <- fitted(glm_logit)\n",
    "yhat_test <- predict(glm_logit, grad_test)\n",
    "\n",
    "head(yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yhat_train\n",
       " 0  1 \n",
       "73 27 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert probabilities to \"success\"/\"failure\"s.\n",
    "p_thresh <- 0.5\n",
    "\n",
    "yhat_train <- (yhat_train > p_thresh) * 1\n",
    "yhat_test <- (yhat_test > p_thresh) * 1\n",
    "\n",
    "table(yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.75"
      ],
      "text/latex": [
       "0.75"
      ],
      "text/markdown": [
       "0.75"
      ],
      "text/plain": [
       "[1] 0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training classification error\n",
    "mean(yhat_train == grad_train$admit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.69"
      ],
      "text/latex": [
       "0.69"
      ],
      "text/markdown": [
       "0.69"
      ],
      "text/plain": [
       "[1] 0.69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing classification error\n",
    "mean(yhat_test == grad_test$admit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression\n",
    "Using GLMs to classify is not limited to binomial response data.  The multinomial distribution can be used to help classify a response variable with more than 2 levels (also known as \"polytomous data\").\n",
    "\n",
    "Recall the formal definition of a K-class multinomial logistic regression:\n",
    "\n",
    "$Y_i \\sim \\text{iid Mult}(1, \\mathbf{\\pi})$\n",
    "\n",
    "$\\log\\left(\\frac{p_1}{p_K}\\right) = \\beta_{01} + \\beta_{11} x_1 + \\cdots + \\beta_{p1} x_p$\n",
    "\n",
    "$\\log\\left(\\frac{p_2}{p_K}\\right) = \\beta_{02} + \\beta_{12} x_1 + \\cdots + \\beta_{p2} x_p$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$\\log\\left(\\frac{p_{K-1}}{p_K}\\right) = \\beta_{0K-1} + \\beta_{1K-1} x_1 + \\cdots + \\beta_{pK-1} x_p$\n",
    "\n",
    "#### Data\n",
    "This time, I'm using the classic `iris` dataset.  In Python, this public-domain dataset can be found in the `seaborn` plotting package.  The data consist of four flower characteristcs, and the response variable, flower type.  There are 3 classes this can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.9   </td><td>3     </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5     </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t1 & 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t2 & 4.9    & 3      & 1.4    & 0.2    & setosa\\\\\n",
       "\t3 & 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t4 & 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t5 & 5      & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t6 & 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1          5.1         3.5          1.4         0.2  setosa\n",
       "2          4.9         3.0          1.4         0.2  setosa\n",
       "3          4.7         3.2          1.3         0.2  setosa\n",
       "4          4.6         3.1          1.5         0.2  setosa\n",
       "5          5.0         3.6          1.4         0.2  setosa\n",
       "6          5.4         3.9          1.7         0.4  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(iris)\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    setosa versicolor  virginica \n",
       "        50         50         50 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The class is the species variable.  There are 3 species.\n",
    "table(iris$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAFoCAMAAAC8KnXeAAAAP1BMVEUAAAAAujgzMzNNTU1h\nnP9oaGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD4dm3///+QT11r\nAAAACXBIWXMAABJ0AAASdAHeZh94AAAUTklEQVR4nO2dgXaqOhBF00tF0Vp9lv//1kcQTIAh\nTCCJYTx7rdsiMgyafSFMA6gagAiod28AkAnEAlGAWCAKEAtEAWKBKEAsEAW+WMVrQhNlY4Ac\n2GIZlyAVWIYrVlHPiPUP5EFIKwLAFKswPo12WLwP9B9/i1YGfHoKAWKZLlb7P+U/kAP7FKuo\nR3sss9vCHiuPFLsUa+RSDbHyS7FPsYpxiQFi5ZZil2K14FCYdQoZYlk7L4iVR4qdi9VaNTgq\nQqw8UuxXrBkgVh4pIBaPIE3y9/cXO0XgAIj1ImOx/v6cZkGsmAgW6+/PbRbEignE2pIieADE\negGxQgZArBf5ioU+1juRLBbOCt+IaLE+KQXE4iGr1ROkgFg8ZLV6ghQQi4esVk+QAmLxkNXq\nCVJALB6yWj1BCojFQ1arJ0gBsXjIavUEKSAWD1mtniAFxOIhq9UTpIBYPGS1eoIUEIuHrFZP\nkAJi8ZDV6glSyBPr3TctAC3yxGItlfP/dRkpIBYP7wD32KsgKSCWB1LEWhgtGiIFxPJBiFhL\n49sDpFgTAbHWA7HySAGxeEAszwCIxQN9LM8AiMUDZ4WeARCLh6xWT5ACYvGQ1eoJUkAsHrJa\nPUEKiMVDVqsnSAGxeBABC93znFs9QQqIxWMasFRQyLnVE6SAWDwmAYsl0JxbPUEKiMUDYnkG\nQCweEMszAGLxQB/LMwBi8cBZoWcAxOIhq9UTpIBYPGS1eoIUEIuHrFZPkAJi8ZDV6glS7Fcs\n8xR7PP0rwxS7FavY9YMwqTNKiBUTrljFrp+wStbAIFZMmGIVu350L121h1gx2STWP827b1rA\noBPr3ZsRlX2KZT8HGnusiAEftscauLRDsdDHSg9PrCfdi9ePll2IhbPC5PjXsXYplvwUEIuH\nrFZPkGLnYrW9eFTeM0yxX7FmgFh5pIBYPJgBVqc851ZPkAJi8eAF2GWEnFs9QQqIxYMVMCh8\n5tzqCVJALB4QyzMAYvGAWJ4BEIsH+lieARCLB84KPQMgFg9ZrZ4gBcTiIavVE6SAWDxktXqC\nFBCLh6xWT5ACYvF4BbBvs02kCHwXkTeJ9f397ZgHsfzoA/gPBpimCH3fo/eI9f09NcuaB7H8\n6AI8HmUySbEUuw+xvr+nZtnzIJYfEKsDYjmBWGsjIJYT9LFWR6CP5QJnhesjcFboIOcKkIwU\n8sR697XloEWeWKylcv6/LiMFxOIhq9UTpIBYPJyddzMP47Eglh+ucoOZhxGkEMsTR4HUzMOY\nd4jlu1qI5RkAsXhALM8AiMUDfSzPAIjFA2eFngEQi4esVk+QYqEdHpdjocrL4urUZiH6FW1d\nAcTKI4W7HW6FaikeC6uDWEgxDHC3w0GdGqXupaq8t2glEEtICnc7dDuih/7d/Duq8t6+PqnW\nuMa5oyqqfkEz+1yow/Lxk8y4KsrCUyznECmyU84l51a3oEZXhUjhboejur6mG2n6g2J7gDw0\nE4926tiJ9ZpdtcfPVWaNxaq6gzFbOD+xnIM66TICl32IRY0HDZLC3Q73Zs9T/bR7qcad8lG3\nB8Wz/lFpcSp1qn+73Zk1W6l7M7twrnqGkUCVUjHFcg5Dnyl8ctmFWNQI9jApls4Kzwe9F/qt\ntS23xjS9Rzq0rax3VAfV9ep1w5vZhTpdZ1fpZiRQoc6eK4BYPgFvE6vhVp1K9dP3t567p34n\n8tqRDGdfm+PX4e67Sc8VjV5697kglk/AO8XS6MOah1iNjQdV/PpuU7ui4ctKLRU6xqCP5RXw\npj6W6tr16c1dHwrL/pinIQ6FPZd1pa1x0LH03PPhrNAv4D1nhZUqm/3Oo3qe+JX1o9R9nkr3\n0n+0Ynrq1u+6zOxC/TazN3felQ13Bahj5ZFioR0OXeX93oqlp+q+yND25fsKg254M/t5Lufb\n7W6BWEJSLLXDpbGpqPQBrzkUlurUHpjuJ9Xuypq+VPP26d51tszsqlDFKq9QeZeSgj+6Idif\nA91ZeIvZD2YqBk9pglh5pMhcrD5pMeywTR+x+iKOWFQ33t2hT3AaCbE8sJMUs32s1GJRhQd3\nCSJF4QtieWAnuVheEX94LAa/emKIRZVK3UXTNSXVTxUrDR6V914s08X6p4lwH4JOk5kpd8Sn\nkrlYs5j+eoJH92KP5R+Qs1gLdayCnEYfK48UEIsFzgp9A3IWS9P+rfBeHodz7cMfnmKfZYrM\nxTr2fwUfmjUWC0+xzy5F5mLZg+5t+jPBoh5W4SFWLikyF6tUz0PhaI/lAGLlkSJzse79hY3s\nUVnbx2NR3W5u592a59skX19fk3lmsBQ5bApi8Rl33h/VQanDmT+OdPMIUqpQwC032PM8m+Tr\na2qWGd5JD/SEWHzSDpsh6phUaZNbIB3M82uSr6+pWWZA+szQdIjFB2IZIFZAhgXS2lUgpYFY\nDrIUi1NK3j4AIq1Y6GMFj/AWi/oeJ4QVaxU4K3SQoVjUnr9+/jnP/LZ/Tn+zsJc8nH5u7MAe\n1LHySLFNLNX9sH9T8/g7oskfoY/nq9c1qxArjxQBxKodYo0XXMRe8HE9l8/y6OnC3nVBrDxS\nbOxjdQe51yX3tbWHsq6/5x8LJwveLqcC1xXuL8Xms0L1kqkeifW63UO9co/14nZ6t1juLnuQ\nFO4Abuc98J0Y3lrHUrNibexjtWSxx3IXGYKkcAdwyw2h7x3zLrG4nfd1h8L7T1V6d+BTjXkP\nnMIdwC2QLt2VaDdijUsKc+WG9WeF3iUHiCVCrOAMxfIY1dADsSAWRZZ7LPSx/ANyFqvpYz1v\no1Ser7EG+jHBWaFvQM5itdz0Q1fefVYorciUIEX2YmluR4i1txTZi+Vdx3r3TQtAS85ivf5W\n6NODxx4rjxQ5i9UVR38xumGHKXIWK/R4LGoUHvN0j30uaOA1iTW8j9mGC4P/psuZxdxjCUlE\nirWK+Q9EjRtmFqj41SsDq0nsAcm8NlwarjxZzizmHv1MI12sAGeF1JUOzJK6R73dwGmSwSUU\nrDZcvMBivJxZzH29xgwQqwdiDZeDWE/iHQohFsTaAvpYo+XQx2qJefkXzgpxVvicdt8qkgZ1\nrDxSQCweslo9QYqcxVoFxMojRTyxCEUY1kAsISn4YlE9P09WiFVFPhQSvXKyP59Pq1u9bd5A\nP3dAmI2aBrDFos5VffEXq4rcxyLqCHQFIhux7PoAa2iyOyDMRhEBXLGo6lp/Waq5SKdW9ovp\nRTz1aNEpo5mFupXq/igV+8nl4e6PRVVUPYgj1qCiOYkg6p3ugDAbRQUEEet1CeHoukLiEsPB\nogSj2c1iZ3WtH/pJ0zwglgCx+gtUyRvLjC6QplwjmIp11Y+Ue8cd/SDWmogwfaxeLOueIOZG\nISHEOqqfuzrUv+hj9UjrY82cFarBLT9ev+09Wb1FLG1UOzz5xN1MnBX6BYTZqGnA1kvsSbEG\nFtVbxKqvh7o+KVWxtwh1rDxSbC2QKvvH+IWr8+5a3xYgVh4pgohllxPY5QbX+jYAsfJIkfvf\nCttHnpTn8WL2E7/w9K8cU2Qu1sxDmnwehOndF6cCuOOxqOFVZh5x/kONrloYIkXEmhW71/La\nJnYy74idiPV6rNzwrNBDLO/qARXAHUFKDQg184iKDTUedGlQ5zTWrHhhLf9NQrkjSNkROxFr\n7kGYGpZY3vVOKoCoo5JQQ9jNPKLGbKqX1JQbs5xZ8dJa/ptPO4N3xE7Eej26l/iTzlSsf5rh\nLQQ6JwZTTqgAZuh/XQvPzCPe7VppZsqNWc6smLmWLcm4EZmLVZ/Kmz4UluP7Y5kOO/ZY2GMx\nmBwK54cno4/1jT4Wm+Bi4azQAmeFC/iUG0hyrgDJSAGxeMhq9QQpshfrom8TWY5vZ9RX2wtr\n+gnEyiNF5mI92tsmN12tOEOT+chq9QQpMhfrpCpdJP0JMjR5y/CqDU3i7LwbqMWoTrn7/IC5\nTRRkp1yqWEqZfzz8bgrCZr1YznKDgVqMKiO4KxrMbaKgywgQq8frNkZ8VovlLJAaqMWowqe7\nBrvhU8wUPqWK1R0KqwBDkyEWxDI86GEzDiDWuk/xYWLV9bk5LzxU/Dtyo4/F2CaKHPtYYxvU\n/Fu+q/IGZ4WcbaJ411kh9y9Y28CYdyEp2GJx/+a+kYFYj0q//CnUkd3FgliZpOCKRfbt+kue\n28u5uvuBvC7zmr1Wx8lggUIv/9t23t/6TOg1AZ+eIphYxqLafk1cXejEfv+iysanQ6nvZsS+\nYhVi5ZFik1j11B7CJjWOcGG/Xyp9IYUuYT1UMRsxIj+x3MOmiABqMWrK3WWn3mV+CmvriIjA\n47HIPpanWIxjof1+u/BPu7Pa8c1t3QM9iQBqMWrKXWQg3+V9CnvrphHBR5CSq1OWW4tiua6A\ntpfpKfSLSukhM/sVi6pt0rv/SbnTLEZNucui9LusTzHYuknETCF1kiLUTUEYYnn3sdpLdA6H\nWnfg49x4jQ/Eot4lCFUgnfTMFS2WMu8ur6/j0nSvrurcdLFKffM1HhBrtBbvT5GhWINyQ23Z\n9Co3eO2x2j8U6kKDUgf2BuUmFvpYodhWOx9E3w7P0qjH7bHyEwtnhdth9aIYq9hCfmJ9Zoqw\neyyfG7LPrGHrJkCsPFJkP2zGl3+LdxUAKZAnFmupnP+vy0gBsXgQAQuDoJyDpYgOsHt1VOcd\nYnmwH7GWhm26hncSp+zu1VHlBojlw27EctcnTQS1HFFkdK+OKpBSG7UExFoPxHIAsdYDsRxA\nrPWgj8XYpogREMsPnBV6BkAsHrKKTAlSQCweslo9QQqIxUNWqydIAbF4yGr1BCkgFg/vAKov\nbuY5B0u5Bzxt2aggYr1hPFYApIhFVQ/MPOfwzoUhmhs2KohYbxtBug0hYlH1TjPPOSB9aVD5\n+o0KIlaqMe+hgVgQKwoQC2JFQYhY6GNBLB44K3yBs0IHOVeAZKSAWDxktXqCFBCLh6xWT5AC\nYvGQ1eoJUuxULPuJX8Xg8V8QK48U+xRr8IzC4c3+QopFjalbGN73gjoD5J1PLW2KM2JDijAR\nEGsZahTw0oDkHqpmxawALW2KK2JDikAR+xarpRj86gknFnXdwuIlFPV4OTPFrVkvbYojYkOK\nUBGCxDJdrH+aYHcf6FppMK/TZDHWLGemqPVt2JQtyyVgv2INjoQxOu/YY62J2P8eq6BfoI+F\nPhYFW6xi5hXOCtemCBOxd7GK4RTqWLml2KlYxXDSegmx8kixT7GK/lSwqIdVeIiVS4p9iuUA\nYuWRAmI5sDrqQQb6uYFYMclJLLu04PsFc8sSvtu0LQJirSecWINiqOcXzC2k+m7TxgiItR6I\nlUcKiDULxNqSAmLNgz4WxDLgrDCPFBCLh6xWT5ACYvGQ1eoJUkAsHrJaPUEKiMVDVqsnSPGZ\nYv39/fmt1b8rHrjzTq4OYvFJItbfn6dZ/sWDwOUGenUQi08Ksf7+PM3yL3cGLpDOrA5i8YFY\nHquDWHwglsfqIBYf9LE8Vgex+OCs0GN1EIsP6lhCUsgT693XloMWeWKxlsr5/7qMFBCLh6xW\nT5ACYvGI3yTUxfELZwAZfgqI5Un0JqFu57FUs8jvU0As39XGbhLqBkSLVdbsPkUNsXxXC7E8\nAyAWD4jlGQCxeKCP5RkAsXjgrNAzAGLxkFVkSpACYvGQ1eoJUkAsHrJaPUEKiMVDVqsnSAGx\neMhq9QQpIBYPWa2eIAXE4iGr1ROkgFg8ZLV6ghQQi4esVk+QAmLxkNXqCVJALB6yWj1BCojF\nQ1arJ0gBsXjIavUEKSAWD1mtniDFTsWyH8yEhzTlmGKfYtnPKEzyvEL/C+azbvUEKSAWhxW3\n+Mi61ROk2KdYLenEWnNToqxbPUEKUWL900S4D0EnVoQ1y2W/YhX2b+yxcksBsTigj+UdsFux\nisEEzgpzS7FXsYrhVGyxhLV6ghQ7FasYTUKs3FLsU6yi6MrtRY3Ke54p9imWA4iVRwqIxUNW\nqydIAbF4yGr1BCkgFg9ZrZ4gBcTiIavVE6SAWDxktXqCFBCLh6xWT5BCnlggD0LYEJDNYkUi\nwfeEFDGBWEgRBYiFFFHIVSywcyAWiALEAlGAWCAKEAtEAWKBKOQpVjEcpRorRfwM+/8Ua8lU\nrDQZErRJ3BSpPsUKIFb8NJFXD7HY7H5PkioNxPIjReekTtE7iZ0BYvmR4Pt6XcwWN0v0BOi8\neyOh24s9VoYIECuNuBCLTYpDYfwUECs7EvR/IFZkshQrRZ80Rbc3wWknOu/gs4BYIAoQC0QB\nYoEoQCwQBYgFogCxQBQgFogCxAJRgFggCvLEelyOhSovywsqNZ5Y4FJ4LPzpiPuaboVqKR5L\nS3qL1S4HsXiI+5oO6tQodS9VtbQkxIqJuK+pa/hH+/txUq1neu5RlXf9zu+x2Z1VNS2WFXA/\nPhfTkh6uzTJ6P9guXHVvAAfixDqqq3nRHhYPtdbh1B0er88jZUWLZQUU3WKP7tj6EuvYvQFc\niBPrXqhD9dPum+qzbv9KXbQO5aNuD48H9dN0xLp9zxMj1ijgogo9r6wf5SugfeOsMh0FlQ/i\nxKof54Pe6/zWWiI9Qx21DrfGuXZfVN+v53JGLDvg3r1z0FN3S6x7jZ7WMiK/oFt1KvWOSXX0\nIrQ/y+G82raEChhNWWsCDsR+QfpgRXlyUofL9Q6xYiPuC1Lq0f3uj2zPV+3xrOyUeDgPhWYe\neSgchgAacV9Qpcqme/WodEep0n3xn6dObQ/8rKd+7b64xlhiB/TvVPplCbE8kfcFHbrK+/1V\nKbi1Yul5tfZkcLR7TvSz7IC6e/dVbqjV8/DavwFcCPyCLqWugLYHxPtJtTswfSgs1aktQrSz\nZsSyA+r+py6Q/uipC8Ti8yFf0GYRULjyBGIth/7qA+gp4NZ8AhBria5Tdg+4NZ8AxFrk0pwO\nnOCVJx8iFkgNxAJRgFggChALRAFigShALBAFiAWiALFAFCAWiML/ZO7SmDK7N/QAAAAASUVO\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAFoCAMAAAC8KnXeAAAAP1BMVEUAAAAAujgzMzNNTU1h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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notice that this variable visually appears to be very easy to predict.\n",
    "ggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n",
    "    theme_bw() +\n",
    "    geom_point(aes(color = Species))\n",
    "\n",
    "ggplot(iris, aes(Petal.Length, Petal.Width)) +\n",
    "    theme_bw() +\n",
    "    geom_point(aes(color = Species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The VGAM Package\n",
    "Everything you need in order to perform multinomial (and cumulative) logistic regression is in the `VGAM` package.  VGAM stands for **v**ector **g**eneralized **a**dditive **m**odels.  The function we'll use here is `vglm()` (vector GLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "n_train <- 100\n",
    "index_train <- sample(1:nrow(iris), n_train, replace = FALSE)\n",
    "\n",
    "iris_train <- iris[index_train,]\n",
    "iris_test <- iris[-index_train,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "vglm(formula = Species ~ Sepal.Length + Sepal.Width, family = multinomial, \n",
       "    data = iris_train)\n",
       "\n",
       "\n",
       "Pearson residuals:\n",
       "                      Min         1Q     Median        3Q     Max\n",
       "log(mu[,1]/mu[,3]) -1.080  5.882e-13  7.926e-13 3.853e-08 0.08078\n",
       "log(mu[,2]/mu[,3]) -3.243 -4.791e-01 -2.441e-06 5.889e-01 2.15419\n",
       "\n",
       "Coefficients: \n",
       "               Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept):1  112.2929   142.0932   0.790  0.42937    \n",
       "(Intercept):2   12.5344     3.6380   3.445  0.00057 ***\n",
       "Sepal.Length:1 -38.9931    48.0339  -0.812  0.41692    \n",
       "Sepal.Length:2  -1.5787     0.5784  -2.729  0.00634 ** \n",
       "Sepal.Width:1   30.8706    43.4151   0.711  0.47705    \n",
       "Sepal.Width:2   -0.9432     1.0492  -0.899  0.36868    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Number of linear predictors:  2 \n",
       "\n",
       "Names of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n",
       "\n",
       "Residual deviance: 76.7433 on 194 degrees of freedom\n",
       "\n",
       "Log-likelihood: -38.3717 on 194 degrees of freedom\n",
       "\n",
       "Number of iterations: 17 \n",
       "\n",
       "Reference group is level  3  of the response"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model.\n",
    "# I am supressing warnings because there are many computational warnings.\n",
    "# The primary one being fitted values being set to 0 or 1.\n",
    "suppressWarnings({\n",
    "    multi_glm <- vglm(Species ~ Sepal.Length + Sepal.Width,\n",
    "                      data = iris_train,\n",
    "                      family = multinomial)\n",
    "})\n",
    "\n",
    "summary(multi_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In object@family@linkinv(predictor, extra = new.extra): fitted probabilities numerically 0 or 1 occurred"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>setosa</th><th scope=col>versicolor</th><th scope=col>virginica</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>100</th><td>1.752642e-11</td><td>7.098762e-01</td><td>2.901238e-01</td></tr>\n",
       "\t<tr><th scope=row>79</th><td>4.609823e-15</td><td>5.809924e-01</td><td>4.190076e-01</td></tr>\n",
       "\t<tr><th scope=row>47</th><td>1.000000e+00</td><td>1.100562e-13</td><td>4.479908e-14</td></tr>\n",
       "\t<tr><th scope=row>113</th><td>5.035555e-27</td><td>2.629984e-01</td><td>7.370016e-01</td></tr>\n",
       "\t<tr><th scope=row>77</th><td>9.944245e-30</td><td>3.011557e-01</td><td>6.988443e-01</td></tr>\n",
       "\t<tr><th scope=row>107</th><td>0.01597185</td><td>0.90518760</td><td>0.07884055</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & setosa & versicolor & virginica\\\\\n",
       "\\hline\n",
       "\t100 & 1.752642e-11 & 7.098762e-01 & 2.901238e-01\\\\\n",
       "\t79 & 4.609823e-15 & 5.809924e-01 & 4.190076e-01\\\\\n",
       "\t47 & 1.000000e+00 & 1.100562e-13 & 4.479908e-14\\\\\n",
       "\t113 & 5.035555e-27 & 2.629984e-01 & 7.370016e-01\\\\\n",
       "\t77 & 9.944245e-30 & 3.011557e-01 & 6.988443e-01\\\\\n",
       "\t107 & 0.01597185 & 0.90518760 & 0.07884055\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 1.7526423366409e-11\n",
       "2. 4.60982294637643e-15\n",
       "3. 0.999999999999845\n",
       "4. 5.03555541380962e-27\n",
       "5. 9.94424453037128e-30\n",
       "6. 0.0159718538901521\n",
       "7. 0.709876206711346\n",
       "8. 0.580992373181541\n",
       "9. 1.10056214193489e-13\n",
       "10. 0.262998410389876\n",
       "11. 0.301155669187455\n",
       "12. 0.905187600449039\n",
       "13. 0.290123793271127\n",
       "14. 0.419007626818454\n",
       "15. 4.47990849676126e-14\n",
       "16. 0.737001589610124\n",
       "17. 0.698844330812545\n",
       "18. 0.078840545660809\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "          setosa   versicolor    virginica\n",
       "100 1.752642e-11 7.098762e-01 2.901238e-01\n",
       "79  4.609823e-15 5.809924e-01 4.190076e-01\n",
       "47  1.000000e+00 1.100562e-13 4.479908e-14\n",
       "113 5.035555e-27 2.629984e-01 7.370016e-01\n",
       "77  9.944245e-30 3.011557e-01 6.988443e-01\n",
       "107 1.597185e-02 9.051876e-01 7.884055e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat_train <- predict(multi_glm, iris_train, type = 'response')\n",
    "yhat_test <- predict(multi_glm, iris_test, type = 'response')\n",
    "head(yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  1 \n",
       "100 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perfect!  Fitted values are indeed probabilities.\n",
    "table(rowSums(yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_class\n",
       " 1  2  3 \n",
       "32 36 32 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the maximum probability row-wise.\n",
    "train_class <- apply(as.matrix(yhat_train), 1, which.max)\n",
    "test_class <- apply(as.matrix(yhat_test), 1, which.max)\n",
    "\n",
    "table(train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_class\n",
       "    setosa versicolor  virginica \n",
       "        32         36         32 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's revert the integer classes back into word-classes.\n",
    "class_lookup <- c('1' = 'setosa',\n",
    "                  '2' = 'versicolor',\n",
    "                  '3' = 'virginica')\n",
    "\n",
    "train_class <- class_lookup[train_class]\n",
    "test_class <- class_lookup[test_class]\n",
    "\n",
    "table(train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified:  82 \n",
      "Total training data:  100 \n",
      "Training classification accuracy rate:  0.82"
     ]
    }
   ],
   "source": [
    "cat('Correctly classified: ', sum(train_class == iris_train$Species), '\\n')\n",
    "cat('Total training data: ', nrow(iris_train), '\\n')\n",
    "cat('Training classification accuracy rate: ', mean(train_class == iris_train$Species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified:  41 \n",
      "Total testing data:  50 \n",
      "Training classification accuracy rate:  0.82"
     ]
    }
   ],
   "source": [
    "cat('Correctly classified: ', sum(test_class == iris_test$Species), '\\n')\n",
    "cat('Total testing data: ', nrow(iris_test), '\\n')\n",
    "cat('Training classification accuracy rate: ', mean(test_class == iris_test$Species))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
